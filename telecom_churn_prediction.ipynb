{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Goal of the project**\n","metadata":{"id":"rBmInmnTejoM"}},{"cell_type":"markdown","source":"The ability to predict that a particular customer is at a high risk of churning, while there is still time to do something about it, represents a huge additional potential revenue source for every online business. Besides the direct loss of revenue that results from a customer abandoning the business, the costs of initially acquiring that customer may not have already been covered by the customer’s spending to date. (In other words, acquiring that customer may have actually been a losing investment.) Furthermore, it is always more difficult and expensive to acquire a new customer than it is to retain a current paying customer.","metadata":{"id":"MtM_wtTdCqeH"}},{"cell_type":"markdown","source":"In this project, we’ll build a contractual churn model for contractual settings.","metadata":{"id":"qRt_NCqMUimT"}},{"cell_type":"markdown","source":"**Load the package**","metadata":{"id":"OvQj42waKLsr"}},{"cell_type":"code","source":"# Importing library\nimport pandas as pd","metadata":{"id":"Ve9jnH44AS-4","execution":{"iopub.status.busy":"2022-08-11T14:44:54.402200Z","iopub.execute_input":"2022-08-11T14:44:54.402977Z","iopub.status.idle":"2022-08-11T14:44:54.407949Z","shell.execute_reply.started":"2022-08-11T14:44:54.402938Z","shell.execute_reply":"2022-08-11T14:44:54.406697Z"},"trusted":true},"execution_count":207,"outputs":[]},{"cell_type":"markdown","source":"**Load the data**","metadata":{"id":"p1OSkCjjARoT"}},{"cell_type":"markdown","source":"For this project I’ve used the [Iranian Churn](https://www.kaggle.com/datasets/royjafari/customer-churn) dataset from the UCI Machine Learning Repository. This is data from a telecoms provider, so we’ll be using it here to create a contractual churn model.","metadata":{"id":"qx6nV-JxIjkF"}},{"cell_type":"code","source":"# Load dataset\ndf = pd.read_csv('../input/customer-churn/Customer Churn.csv')","metadata":{"id":"j3Yk-eM7AoBb","execution":{"iopub.status.busy":"2022-08-11T14:44:54.663008Z","iopub.execute_input":"2022-08-11T14:44:54.663332Z","iopub.status.idle":"2022-08-11T14:44:54.678529Z","shell.execute_reply.started":"2022-08-11T14:44:54.663303Z","shell.execute_reply":"2022-08-11T14:44:54.677444Z"},"trusted":true},"execution_count":208,"outputs":[]},{"cell_type":"code","source":"# Rename Pandas columns to lower case\ndf.columns = df.columns.str.lower()","metadata":{"id":"4-S-GCO4-Z00","execution":{"iopub.status.busy":"2022-08-11T14:44:54.892774Z","iopub.execute_input":"2022-08-11T14:44:54.893605Z","iopub.status.idle":"2022-08-11T14:44:54.899546Z","shell.execute_reply.started":"2022-08-11T14:44:54.893561Z","shell.execute_reply":"2022-08-11T14:44:54.898479Z"},"trusted":true},"execution_count":209,"outputs":[]},{"cell_type":"code","source":"# Examine the data\ndf.head()","metadata":{"id":"GcRFPPNEOuRT","outputId":"90ff8a4b-ae72-4589-9993-4094c1079914","execution":{"iopub.status.busy":"2022-08-11T14:44:55.145435Z","iopub.execute_input":"2022-08-11T14:44:55.145763Z","iopub.status.idle":"2022-08-11T14:44:55.164619Z","shell.execute_reply.started":"2022-08-11T14:44:55.145713Z","shell.execute_reply":"2022-08-11T14:44:55.163430Z"},"trusted":true},"execution_count":210,"outputs":[{"execution_count":210,"output_type":"execute_result","data":{"text/plain":"   call  failure  complains  subscription  length  charge  amount  \\\n0              8          0                    38               0   \n1              0          0                    39               0   \n2             10          0                    37               0   \n3             10          0                    38               0   \n4              3          0                    38               0   \n\n   seconds of use  frequency of use  frequency of sms  \\\n0            4370                71                 5   \n1             318                 5                 7   \n2            2453                60               359   \n3            4198                66                 1   \n4            2393                58                 2   \n\n   distinct called numbers  age group  tariff plan  status  age  \\\n0                       17          3            1       1   30   \n1                        4          2            1       2   25   \n2                       24          3            1       1   30   \n3                       35          1            1       1   15   \n4                       33          1            1       1   15   \n\n   customer value         fn        fp  churn  \n0         197.640   177.8760   69.7640      0  \n1          46.035    41.4315   60.0000      0  \n2        1536.520  1382.8680  203.6520      0  \n3         240.020   216.0180   74.0020      0  \n4         145.805   131.2245   64.5805      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>call  failure</th>\n      <th>complains</th>\n      <th>subscription  length</th>\n      <th>charge  amount</th>\n      <th>seconds of use</th>\n      <th>frequency of use</th>\n      <th>frequency of sms</th>\n      <th>distinct called numbers</th>\n      <th>age group</th>\n      <th>tariff plan</th>\n      <th>status</th>\n      <th>age</th>\n      <th>customer value</th>\n      <th>fn</th>\n      <th>fp</th>\n      <th>churn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8</td>\n      <td>0</td>\n      <td>38</td>\n      <td>0</td>\n      <td>4370</td>\n      <td>71</td>\n      <td>5</td>\n      <td>17</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>30</td>\n      <td>197.640</td>\n      <td>177.8760</td>\n      <td>69.7640</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>39</td>\n      <td>0</td>\n      <td>318</td>\n      <td>5</td>\n      <td>7</td>\n      <td>4</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>25</td>\n      <td>46.035</td>\n      <td>41.4315</td>\n      <td>60.0000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10</td>\n      <td>0</td>\n      <td>37</td>\n      <td>0</td>\n      <td>2453</td>\n      <td>60</td>\n      <td>359</td>\n      <td>24</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>30</td>\n      <td>1536.520</td>\n      <td>1382.8680</td>\n      <td>203.6520</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10</td>\n      <td>0</td>\n      <td>38</td>\n      <td>0</td>\n      <td>4198</td>\n      <td>66</td>\n      <td>1</td>\n      <td>35</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>15</td>\n      <td>240.020</td>\n      <td>216.0180</td>\n      <td>74.0020</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>0</td>\n      <td>38</td>\n      <td>0</td>\n      <td>2393</td>\n      <td>58</td>\n      <td>2</td>\n      <td>33</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>15</td>\n      <td>145.805</td>\n      <td>131.2245</td>\n      <td>64.5805</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Overview of all variables, their datatypes\ndf.info()","metadata":{"id":"ggHEXgt5O3c6","outputId":"a07d9011-1e7d-41e6-e027-e3048c807a34","execution":{"iopub.status.busy":"2022-08-11T14:44:55.362942Z","iopub.execute_input":"2022-08-11T14:44:55.363493Z","iopub.status.idle":"2022-08-11T14:44:55.379670Z","shell.execute_reply.started":"2022-08-11T14:44:55.363461Z","shell.execute_reply":"2022-08-11T14:44:55.378786Z"},"trusted":true},"execution_count":211,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3150 entries, 0 to 3149\nData columns (total 16 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   call  failure            3150 non-null   int64  \n 1   complains                3150 non-null   int64  \n 2   subscription  length     3150 non-null   int64  \n 3   charge  amount           3150 non-null   int64  \n 4   seconds of use           3150 non-null   int64  \n 5   frequency of use         3150 non-null   int64  \n 6   frequency of sms         3150 non-null   int64  \n 7   distinct called numbers  3150 non-null   int64  \n 8   age group                3150 non-null   int64  \n 9   tariff plan              3150 non-null   int64  \n 10  status                   3150 non-null   int64  \n 11  age                      3150 non-null   int64  \n 12  customer value           3150 non-null   float64\n 13  fn                       3150 non-null   float64\n 14  fp                       3150 non-null   float64\n 15  churn                    3150 non-null   int64  \ndtypes: float64(3), int64(13)\nmemory usage: 393.9 KB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Examine the data**","metadata":{}},{"cell_type":"markdown","source":"As we’ll see from examining the value_counts( ) of the target variable column, this dataset is imbalanced. The positive class (customers who churned) comprise about 16% of the dataset. This is the norm for customer churn datasets in non contractual and contractual settings, but does introduce some challenges we need to handle, otherwise the model may fail to make accurate predictions.","metadata":{}},{"cell_type":"code","source":"df['churn'].value_counts()","metadata":{"id":"K8PN-GxmBniq","outputId":"2594c468-807a-4455-b50e-224c13b53f17","execution":{"iopub.status.busy":"2022-08-11T14:44:55.604208Z","iopub.execute_input":"2022-08-11T14:44:55.604856Z","iopub.status.idle":"2022-08-11T14:44:55.614129Z","shell.execute_reply.started":"2022-08-11T14:44:55.604816Z","shell.execute_reply":"2022-08-11T14:44:55.613020Z"},"trusted":true},"execution_count":212,"outputs":[{"execution_count":212,"output_type":"execute_result","data":{"text/plain":"0    2655\n1     495\nName: churn, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"**Check for missing values**","metadata":{"id":"menw3-ibCGX9"}},{"cell_type":"markdown","source":"This dataset has probably been pre-cleansed, as df.isnull( ).sum( ) shows us that we don’t have any null values to deal with at all, which will save us a step later.","metadata":{"id":"QWjAUDhWUHUL"}},{"cell_type":"code","source":"# Check for missing values\ndf.isnull().sum()","metadata":{"id":"28MsfXzToekQ","outputId":"c7a852dc-e6e3-4b92-af59-1309d453aa06","execution":{"iopub.status.busy":"2022-08-11T14:44:55.963254Z","iopub.execute_input":"2022-08-11T14:44:55.964208Z","iopub.status.idle":"2022-08-11T14:44:55.974407Z","shell.execute_reply.started":"2022-08-11T14:44:55.964161Z","shell.execute_reply":"2022-08-11T14:44:55.973144Z"},"trusted":true},"execution_count":213,"outputs":[{"execution_count":213,"output_type":"execute_result","data":{"text/plain":"call  failure              0\ncomplains                  0\nsubscription  length       0\ncharge  amount             0\nseconds of use             0\nfrequency of use           0\nfrequency of sms           0\ndistinct called numbers    0\nage group                  0\ntariff plan                0\nstatus                     0\nage                        0\ncustomer value             0\nfn                         0\nfp                         0\nchurn                      0\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"**Create the training and test data**","metadata":{"id":"hCif0_YGoBiw"}},{"cell_type":"markdown","source":"Now we have prepared our feature set, we need to define X and y. The X feature set will include all the features we created above, minus the target variable, so we don’t give the model the answer. The y data will comprise the target variable alone.","metadata":{}},{"cell_type":"code","source":"X = df.drop('churn', axis = 1) ","metadata":{"id":"Db5V-6K9Aqau","execution":{"iopub.status.busy":"2022-08-11T14:44:56.298664Z","iopub.execute_input":"2022-08-11T14:44:56.299358Z","iopub.status.idle":"2022-08-11T14:44:56.305248Z","shell.execute_reply.started":"2022-08-11T14:44:56.299325Z","shell.execute_reply":"2022-08-11T14:44:56.304169Z"},"trusted":true},"execution_count":214,"outputs":[]},{"cell_type":"code","source":"y = df['churn']","metadata":{"id":"YVAA05vTnjL9","execution":{"iopub.status.busy":"2022-08-11T14:44:56.530229Z","iopub.execute_input":"2022-08-11T14:44:56.530530Z","iopub.status.idle":"2022-08-11T14:44:56.535358Z","shell.execute_reply.started":"2022-08-11T14:44:56.530502Z","shell.execute_reply":"2022-08-11T14:44:56.534045Z"},"trusted":true},"execution_count":215,"outputs":[]},{"cell_type":"markdown","source":"We’ll now randomly split the data into a training and test dataset using the train_test_split( ) function. We’ll assign 30% of the data to the test group, which will be held out of training, and we’ll use the stratify option to ensure the target variable is present in equal proportions in each group. The random_state value ensures we get reproducible results each time we run the code.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"id":"onwsmsYdCoh0","execution":{"iopub.status.busy":"2022-08-11T14:44:56.690908Z","iopub.execute_input":"2022-08-11T14:44:56.691974Z","iopub.status.idle":"2022-08-11T14:44:56.697462Z","shell.execute_reply.started":"2022-08-11T14:44:56.691937Z","shell.execute_reply":"2022-08-11T14:44:56.696303Z"},"trusted":true},"execution_count":216,"outputs":[]},{"cell_type":"code","source":"# Isolate X and y variables, and perform train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify = y)","metadata":{"id":"-LrXeaAmCHXj","execution":{"iopub.status.busy":"2022-08-11T14:44:57.030768Z","iopub.execute_input":"2022-08-11T14:44:57.031968Z","iopub.status.idle":"2022-08-11T14:44:57.041729Z","shell.execute_reply.started":"2022-08-11T14:44:57.031918Z","shell.execute_reply":"2022-08-11T14:44:57.040759Z"},"trusted":true},"execution_count":217,"outputs":[]},{"cell_type":"markdown","source":"**Create a model pipeline**","metadata":{"id":"TatYScbcxOVn"}},{"cell_type":"markdown","source":"Next, we’ll create a model pipeline. This will handle the encoding of our data using the ColumnTransformer( ) feature.\n\nI’ve also added some basic feature selection via SelectKBest( ), and have used the Synthetic Minority Oversampling Technique and Edited Nearest Neighbor (SMOTE-ENN) method to better handle class imbalance. ","metadata":{}},{"cell_type":"code","source":"import time\nfrom sklearn.compose import ColumnTransformer\n\nfrom imblearn.pipeline import Pipeline as imbpipeline\nfrom imblearn.combine import SMOTEENN\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score","metadata":{"id":"41hl-QKjFvEp","execution":{"iopub.status.busy":"2022-08-11T14:44:57.238576Z","iopub.execute_input":"2022-08-11T14:44:57.239593Z","iopub.status.idle":"2022-08-11T14:44:57.245247Z","shell.execute_reply.started":"2022-08-11T14:44:57.239554Z","shell.execute_reply":"2022-08-11T14:44:57.244117Z"},"trusted":true},"execution_count":218,"outputs":[]},{"cell_type":"code","source":"def get_pipeline(X, model):\n\n    numeric_columns = list(X.select_dtypes(exclude = ['object']).columns.values.tolist())    \n    \n    preprocessor = ColumnTransformer(transformers = [('numeric', 'passthrough', numeric_columns)], remainder = 'passthrough')\n\n    bundled_pipeline = imbpipeline(steps = [('preprocessor', preprocessor),\n                                            ('smote', SMOTEENN(random_state = 42)),\n                                            ('scaler', RobustScaler()),\n                                            ('feature_selection', SelectKBest(score_func = mutual_info_classif, k = 13)),\n                                            ('model', model)])\n    \n    return bundled_pipeline","metadata":{"id":"Q_PCjzY_Fu2y","execution":{"iopub.status.busy":"2022-08-11T14:44:57.460938Z","iopub.execute_input":"2022-08-11T14:44:57.461255Z","iopub.status.idle":"2022-08-11T14:44:57.467455Z","shell.execute_reply.started":"2022-08-11T14:44:57.461221Z","shell.execute_reply":"2022-08-11T14:44:57.466444Z"},"trusted":true},"execution_count":219,"outputs":[]},{"cell_type":"markdown","source":"**Select the best model**","metadata":{"id":"6GcVc1Rpx1Kp"}},{"cell_type":"markdown","source":"Rather than simply selecting a single model, or repeating our code manually on a range of models, we can create another function to automatically test a wide range of possible models to determine the best one for our needs. To do this we first create a dictionary containing some a selection of base classifiers.\n\nWe will create a Pandas dataframe into which we will store the data. Then we will loop over each of the models, fit it using the X_train and y_train data, then generate predictions from X_test and calculate the mean ROC/AUC score from 5 rounds of cross-validation. That will give us the ROC/AUC score for the X_test data, plus the average ROC/AUC score for the training dataset.","metadata":{"id":"V7Gb30iAzqUo"}},{"cell_type":"code","source":"from xgboost import XGBClassifier, XGBRFClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, HistGradientBoostingClassifier\nfrom catboost import CatBoostClassifier","metadata":{"id":"Zi1Vm0BsFvPr","execution":{"iopub.status.busy":"2022-08-11T14:44:57.673968Z","iopub.execute_input":"2022-08-11T14:44:57.675987Z","iopub.status.idle":"2022-08-11T14:44:57.681075Z","shell.execute_reply.started":"2022-08-11T14:44:57.675956Z","shell.execute_reply":"2022-08-11T14:44:57.680028Z"},"trusted":true},"execution_count":220,"outputs":[]},{"cell_type":"code","source":"def select_model(X, y, pipeline = None):\n\n  classifiers = {}\n  classifiers.update({'XGBClassifier': XGBClassifier(random_state = 42)})\n  classifiers.update({'XGBRFClassifier': XGBRFClassifier(random_state = 42)})\n  classifiers.update({'LGBMClassifier': LGBMClassifier(random_state = 42)})\n  classifiers.update({'DecisionTreeClassifier': DecisionTreeClassifier(random_state = 42)})\n  classifiers.update({'RandomForestClassifier': RandomForestClassifier(random_state = 42)})\n  classifiers.update({'ExtraTreesClassifier': ExtraTreesClassifier(random_state = 42)})\n  classifiers.update({'GradientBoostingClassifier': GradientBoostingClassifier(random_state = 42)})    \n  classifiers.update({'BaggingClassifier': BaggingClassifier(random_state = 42)})\n  classifiers.update({'AdaBoostClassifier': AdaBoostClassifier(random_state = 42)})\n  classifiers.update({'HistGradientBoostingClassifier': HistGradientBoostingClassifier(random_state = 42)})\n  classifiers.update({'CatBoostClassifier': CatBoostClassifier(silent = True, random_state = 42)})\n\n  df_models = pd.DataFrame(columns = ['model', 'run_time', 'roc_auc_cv', 'roc_auc'])\n\n  for key in classifiers:\n\n      print('*', key)\n\n      start_time = time.time()\n      \n      pipeline = get_pipeline(X_train, classifiers[key])\n\n      cv = cross_val_score(pipeline, X, y, cv = 5, scoring = 'roc_auc', n_jobs = -1)\n\n      pipeline.fit(X_train, y_train)\n      y_pred = pipeline.predict(X_test)\n\n      row = {'model': key,\n             'run_time': format(round((time.time() - start_time) / 60, 2)),\n             'roc_auc_cv': cv.mean(),\n             'roc_auc': roc_auc_score(y_test, y_pred)}\n\n      df_models = df_models.append(row, ignore_index = True)\n\n  df_models = df_models.sort_values(by = 'roc_auc', ascending = False)\n      \n  return df_models","metadata":{"id":"tPxOY4toFuqR","execution":{"iopub.status.busy":"2022-08-11T14:44:57.969169Z","iopub.execute_input":"2022-08-11T14:44:57.969507Z","iopub.status.idle":"2022-08-11T14:44:57.981215Z","shell.execute_reply.started":"2022-08-11T14:44:57.969477Z","shell.execute_reply":"2022-08-11T14:44:57.979986Z"},"trusted":true},"execution_count":221,"outputs":[]},{"cell_type":"markdown","source":"The top performer was HistGradientBoostingClassifier( ), which generated a ROC/AUC score of 0.942. We’ll select this model as our best one and examine the results in a bit more detail to see how well it works.","metadata":{}},{"cell_type":"code","source":"models = select_model(X_train, y_train)","metadata":{"id":"ZhysNV4NTjMa","outputId":"e2b1445b-fd2c-464b-9734-f9fa69587685","execution":{"iopub.status.busy":"2022-08-11T14:44:58.173423Z","iopub.execute_input":"2022-08-11T14:44:58.173718Z","iopub.status.idle":"2022-08-11T14:45:53.271323Z","shell.execute_reply.started":"2022-08-11T14:44:58.173691Z","shell.execute_reply":"2022-08-11T14:45:53.270302Z"},"trusted":true},"execution_count":222,"outputs":[{"name":"stdout","text":"* XGBClassifier\n* XGBRFClassifier\n* LGBMClassifier\n* DecisionTreeClassifier\n* RandomForestClassifier\n* ExtraTreesClassifier\n* GradientBoostingClassifier\n* BaggingClassifier\n* AdaBoostClassifier\n* HistGradientBoostingClassifier\n* CatBoostClassifier\n","output_type":"stream"}]},{"cell_type":"code","source":"models.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T14:45:53.273538Z","iopub.execute_input":"2022-08-11T14:45:53.273928Z","iopub.status.idle":"2022-08-11T14:45:53.286146Z","shell.execute_reply.started":"2022-08-11T14:45:53.273890Z","shell.execute_reply":"2022-08-11T14:45:53.285043Z"},"trusted":true},"execution_count":223,"outputs":[{"execution_count":223,"output_type":"execute_result","data":{"text/plain":"                             model run_time  roc_auc_cv   roc_auc\n9   HistGradientBoostingClassifier     0.05    0.966518  0.942233\n2                   LGBMClassifier     0.03    0.966288  0.936972\n10              CatBoostClassifier     0.53    0.966784  0.935090\n0                    XGBClassifier     0.05    0.964216  0.932725\n7                BaggingClassifier     0.02    0.949656  0.931326\n4           RandomForestClassifier     0.05    0.959437  0.930940\n5             ExtraTreesClassifier     0.03    0.966763  0.925824\n3           DecisionTreeClassifier     0.01    0.880014  0.914917\n6       GradientBoostingClassifier     0.07    0.955135  0.913133\n8               AdaBoostClassifier     0.03    0.947986  0.903867","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>run_time</th>\n      <th>roc_auc_cv</th>\n      <th>roc_auc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9</th>\n      <td>HistGradientBoostingClassifier</td>\n      <td>0.05</td>\n      <td>0.966518</td>\n      <td>0.942233</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LGBMClassifier</td>\n      <td>0.03</td>\n      <td>0.966288</td>\n      <td>0.936972</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>CatBoostClassifier</td>\n      <td>0.53</td>\n      <td>0.966784</td>\n      <td>0.935090</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>XGBClassifier</td>\n      <td>0.05</td>\n      <td>0.964216</td>\n      <td>0.932725</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>BaggingClassifier</td>\n      <td>0.02</td>\n      <td>0.949656</td>\n      <td>0.931326</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>RandomForestClassifier</td>\n      <td>0.05</td>\n      <td>0.959437</td>\n      <td>0.930940</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>ExtraTreesClassifier</td>\n      <td>0.03</td>\n      <td>0.966763</td>\n      <td>0.925824</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DecisionTreeClassifier</td>\n      <td>0.01</td>\n      <td>0.880014</td>\n      <td>0.914917</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>GradientBoostingClassifier</td>\n      <td>0.07</td>\n      <td>0.955135</td>\n      <td>0.913133</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>AdaBoostClassifier</td>\n      <td>0.03</td>\n      <td>0.947986</td>\n      <td>0.903867</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Examine the performance of the best model**","metadata":{"id":"OTKLBmf7EJOk"}},{"cell_type":"markdown","source":"By re-running the get_pipeline( ) function on our selected model, the HistGradientBoostingClassifier( ), we can generate predictions and assess their accuracy on the test data.","metadata":{"id":"3BnfFLdYGqQD"}},{"cell_type":"code","source":"selected_model = HistGradientBoostingClassifier(random_state = 42)\nbundled_pipeline = get_pipeline(X_train, selected_model)\nbundled_pipeline.fit(X_train, y_train)\ny_pred = bundled_pipeline.predict(X_test)","metadata":{"id":"DvE_mGcyGxFh","execution":{"iopub.status.busy":"2022-08-11T14:45:53.287820Z","iopub.execute_input":"2022-08-11T14:45:53.288510Z","iopub.status.idle":"2022-08-11T14:45:53.925367Z","shell.execute_reply.started":"2022-08-11T14:45:53.288472Z","shell.execute_reply":"2022-08-11T14:45:53.924590Z"},"trusted":true},"execution_count":224,"outputs":[]},{"cell_type":"markdown","source":"**Examine the predictions**","metadata":{"id":"gt8crs94EZ0K"}},{"cell_type":"markdown","source":"The classification_report( ) function is also worth running on the data. For each class, this effectively shows how good the model was at predicting each class. The closer to 1 the better the performance.","metadata":{"id":"8QprSBTJ0L9h"}},{"cell_type":"code","source":"from sklearn.metrics import classification_report","metadata":{"id":"zDDzHrZsMbE7","execution":{"iopub.status.busy":"2022-08-11T14:45:53.930100Z","iopub.execute_input":"2022-08-11T14:45:53.931987Z","iopub.status.idle":"2022-08-11T14:45:53.936945Z","shell.execute_reply.started":"2022-08-11T14:45:53.931953Z","shell.execute_reply":"2022-08-11T14:45:53.936042Z"},"trusted":true},"execution_count":225,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-08-11T14:45:53.939685Z","iopub.execute_input":"2022-08-11T14:45:53.940134Z","iopub.status.idle":"2022-08-11T14:45:53.955906Z","shell.execute_reply.started":"2022-08-11T14:45:53.940099Z","shell.execute_reply":"2022-08-11T14:45:53.954767Z"},"trusted":true},"execution_count":226,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.99      0.94      0.96       797\n           1       0.74      0.95      0.83       148\n\n    accuracy                           0.94       945\n   macro avg       0.87      0.94      0.90       945\nweighted avg       0.95      0.94      0.94       945\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let’s unpack those results a little bit…","metadata":{"id":"jvsr4ZGhG5MK"}},{"cell_type":"markdown","source":"**Recall**\n","metadata":{"id":"Zsf5Dhl2FCeL"}},{"cell_type":"markdown","source":"A churn class recall of 0.95 means that the model was able to catch 95% of the actual churn cases. This is the measure we really care about, because we want to miss as few of the true churn cases as possible.","metadata":{"id":"1emBwbX6FLOj"}},{"cell_type":"markdown","source":"**Precision**","metadata":{"id":"DfBdfva6FPAH"}},{"cell_type":"markdown","source":"Precision of the churn class measures how often the model catches an actual churn case, while also factoring in how often it misclassifies a non-churn case as a churn case. In this case, a churn precision of 0.74 is not a problem because there are no significant consequences of identifying a customer as a churn risk when she isn’t.","metadata":{"id":"5-3PXvabE9tp"}},{"cell_type":"markdown","source":"**F1 score**","metadata":{"id":"V-D6qcB8GAm6"}},{"cell_type":"markdown","source":"The F1 score is the harmonic mean of precision and recall. It helps give us a balanced idea of how the model is performing on the churn class. In this case a churn class F1 score of 0.83 is pretty good. ","metadata":{"id":"Ee8KtFISFwUG"}}]}